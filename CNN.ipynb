{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf33445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 12:04:38.504453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import librosa\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04fe95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos disponíveis:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "GPUs detectadas (1):\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "Detalhes:\n",
      "device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763813081.902914    4329 gpu_device.cc:2020] Created device /device:GPU:0 with 1732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Lista todos os dispositivos visíveis\n",
    "print(\"Dispositivos disponíveis:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Mostra as GPUs detectadas\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nGPUs detectadas ({len(gpus)}):\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "\n",
    "    # Mostra detalhes da GPU em uso\n",
    "    from tensorflow.python.client import device_lib\n",
    "    devices = device_lib.list_local_devices()\n",
    "    print(\"\\nDetalhes:\")\n",
    "    for d in devices:\n",
    "        if d.device_type == 'GPU':\n",
    "            print(d.physical_device_desc)\n",
    "else:\n",
    "    print(\"\\nNenhuma GPU detectada pelo TensorFlow. Ele está usando a CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11771cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35239c0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bf80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(16000, 1), num_classes=10, hidden_layers=5, kernel_size=9):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv_layers = []\n",
    "        self.bn_layers = []\n",
    "        self.pool_layers = []\n",
    "\n",
    "        # Bloco 1\n",
    "        self.conv_layers.append(layers.Conv1D(16, kernel_size=kernel_size, activation='relu', padding='same', input_shape=input_shape))\n",
    "        self.bn_layers.append(layers.BatchNormalization())\n",
    "        self.pool_layers.append(layers.MaxPooling1D(pool_size=4))\n",
    "\n",
    "        # Bloco intermediários\n",
    "        for i in range(1, hidden_layers - 1):\n",
    "            print(f\"layer {i}: {16*2**i}\")\n",
    "            self.conv_layers.append(layers.Conv1D(16*2**i, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "            self.bn_layers.append(layers.BatchNormalization())\n",
    "            self.pool_layers.append(layers.MaxPooling1D(pool_size=4))\n",
    "\n",
    "        # Bloco final\n",
    "        print(f\"layer {hidden_layers - 1}: {16*2**(hidden_layers - 1)}\")\n",
    "        self.conv_last = layers.Conv1D(16*2**(hidden_layers - 1), kernel_size=kernel_size, activation='relu', padding='same')\n",
    "        self.bn_last = layers.BatchNormalization()\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "\n",
    "        # Camadas densas\n",
    "        print(f\"layer {hidden_layers}: {16*2**(hidden_layers)}\")\n",
    "        self.fc1 = layers.Dense(16*2**(hidden_layers), activation='relu')\n",
    "        self.drop = layers.Dropout(0.4)\n",
    "        self.out = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.pool_layers[0](self.bn_layers[0](self.conv_layers[0](inputs), training=training))\n",
    "        for i in range(1, len(self.conv_layers)):\n",
    "            x = self.pool_layers[i](self.bn_layers[i](self.conv_layers[i](x), training=training))\n",
    "        x = self.bn_last(self.conv_last(x), training=training)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efa911",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/metadata/UrbanSound8K.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m AUDIO_PATH = os.path.join(DATASET_PATH, \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Carregar metadados\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m metadata = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(metadata.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acii/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acii/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acii/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acii/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acii/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset/metadata/UrbanSound8K.csv'"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"UrbanSound8K\"\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"metadata/UrbanSound8K.csv\")\n",
    "AUDIO_PATH = os.path.join(DATASET_PATH, \"audio\")\n",
    "\n",
    "# Carregar metadados\n",
    "metadata = pd.read_csv(CSV_PATH)\n",
    "print(metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b730ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sr=16000, max_len=16000):\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    if len(y) > max_len:\n",
    "        y = y[:max_len]\n",
    "    else:\n",
    "        y = np.pad(y, (0, max_len - len(y)))\n",
    "    return np.expand_dims(y, axis=-1)  # (samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4427117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de treino...\n",
      "Carregando dados de teste...\n",
      "Carregando dados de validação...\n",
      "X_train: (7079, 16000, 1), y_train: (7079,)\n",
      "X_test: (816, 16000, 1), y_test: (816,)\n",
      "X_val: (837, 16000, 1), y_val: (837,)\n"
     ]
    }
   ],
   "source": [
    "# Escolher qual fold será usaclassIDdo como validação\n",
    "val_fold = 10\n",
    "test_fold = 9\n",
    "\n",
    "# Separar metadados\n",
    "train_meta = metadata[(metadata[\"fold\"] != val_fold) & (metadata[\"fold\"] != test_fold)]\n",
    "test_meta = metadata[metadata[\"fold\"] == test_fold]\n",
    "val_meta = metadata[metadata[\"fold\"] == val_fold]\n",
    "\n",
    "def build_dataset(meta):\n",
    "    X, y = [], []\n",
    "    for _, row in meta.iterrows():\n",
    "        file_path = os.path.join(AUDIO_PATH, f\"fold{row['fold']}\", row[\"slice_file_name\"])\n",
    "        audio = load_audio(file_path)\n",
    "        X.append(audio)\n",
    "        y.append(row[\"classID\"])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)\n",
    "\n",
    "print(\"Carregando dados de treino...\")\n",
    "X_train, y_train = build_dataset(train_meta)\n",
    "\n",
    "print(\"Carregando dados de teste...\")\n",
    "X_test, y_test = build_dataset(test_meta)\n",
    "\n",
    "print(\"Carregando dados de validação...\")\n",
    "X_val, y_val = build_dataset(val_meta)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1: 32\n",
      "layer 2: 64\n",
      "layer 3: 128\n",
      "layer 4: 256\n",
      "layer 5: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filipe/miniconda3/envs/acii/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.4299 - loss: 1.6819 - val_accuracy: 0.1338 - val_loss: 2.8259 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.5617 - loss: 1.2915 - val_accuracy: 0.2198 - val_loss: 3.9144 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.6288 - loss: 1.1097 - val_accuracy: 0.3990 - val_loss: 1.9367 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.6792 - loss: 0.9690 - val_accuracy: 0.5603 - val_loss: 1.4018 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7101 - loss: 0.8827 - val_accuracy: 0.4444 - val_loss: 2.7270 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7268 - loss: 0.8052 - val_accuracy: 0.4851 - val_loss: 2.2196 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7430 - loss: 0.7628 - val_accuracy: 0.4397 - val_loss: 2.0218 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7637 - loss: 0.7023 - val_accuracy: 0.4671 - val_loss: 2.6477 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8196 - loss: 0.5401 - val_accuracy: 0.4731 - val_loss: 2.4992 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8342 - loss: 0.4925 - val_accuracy: 0.5173 - val_loss: 2.2066 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8425 - loss: 0.4613 - val_accuracy: 0.5639 - val_loss: 1.6723 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.8494 - loss: 0.4434 - val_accuracy: 0.5783 - val_loss: 1.8470 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7efce1eaff20>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciar modelo\n",
    "model = CNN1D(input_shape=(16000, 1), num_classes=10)\n",
    "\n",
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "              tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5)\n",
    "          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3947    0.1500    0.2174       100\n",
      "           1     1.0000    0.3438    0.5116        32\n",
      "           2     0.3713    0.6200    0.4644       100\n",
      "           3     0.5041    0.6100    0.5520       100\n",
      "           4     0.5050    0.5100    0.5075       100\n",
      "           5     0.7000    0.8652    0.7739        89\n",
      "           6     0.4328    0.9355    0.5918        31\n",
      "           7     0.6087    0.6829    0.6437        82\n",
      "           8     0.7955    0.4268    0.5556        82\n",
      "           9     0.6615    0.4300    0.5212       100\n",
      "\n",
      "    accuracy                         0.5392       816\n",
      "   macro avg     0.5974    0.5574    0.5339       816\n",
      "weighted avg     0.5717    0.5392    0.5247       816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Relatório detalhado\n",
    "print(classification_report(y_test, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b07ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class CNN1D_fixed(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(16000,1), num_classes=10, hidden_layers=5, kernel_size=9):\n",
    "        super().__init__()\n",
    "        self.blocks = []\n",
    "        # bloco 1\n",
    "        filters = 16\n",
    "        self.blocks.append(tf.keras.Sequential([\n",
    "            layers.Conv1D(filters, kernel_size, padding='same', activation='relu', input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling1D(pool_size=4)\n",
    "        ]))\n",
    "        # blocos intermediários\n",
    "        for i in range(1, hidden_layers-1):\n",
    "            filters = 16 * (2**i)\n",
    "            self.blocks.append(tf.keras.Sequential([\n",
    "                layers.Conv1D(filters, kernel_size, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling1D(pool_size=4)\n",
    "            ]))\n",
    "        # conv final (mantém dimensionalidade temporal)\n",
    "        final_filters = 16 * (2**(hidden_layers-1))\n",
    "        self.conv_last = layers.Conv1D(final_filters, kernel_size, padding='same', activation='relu')\n",
    "        self.bn_last = layers.BatchNormalization()\n",
    "        self.global_pool = layers.GlobalAveragePooling1D()\n",
    "        # densas\n",
    "        fc_units = 16 * (2**hidden_layers)\n",
    "        self.fc1 = layers.Dense(fc_units, activation='relu')\n",
    "        self.drop = layers.Dropout(0.4)\n",
    "        self.out = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, training=training)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.bn_last(x, training=training)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        return self.out(x)\n",
    "\n",
    "class CNN2D_matched(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(64, 157, 1), num_classes=10, base_filters=16):\n",
    "        super().__init__()\n",
    "        # arquitetura 2D com profundidade similar em parâmetros\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            layers.Conv2D(base_filters, (3,3), padding='same', activation='relu', input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2,2))\n",
    "        ])\n",
    "        self.conv2 = tf.keras.Sequential([\n",
    "            layers.Conv2D(base_filters*2, (3,3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2,2))\n",
    "        ])\n",
    "        self.conv3 = tf.keras.Sequential([\n",
    "            layers.Conv2D(base_filters*4, (3,3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2,2))\n",
    "        ])\n",
    "        # último bloco (aumenta filtros para aproximar parâmetro total)\n",
    "        self.conv_last = tf.keras.Sequential([\n",
    "            layers.Conv2D(base_filters*8, (3,3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.GlobalAveragePooling2D()\n",
    "        ])\n",
    "        # densas alinhadas ao modelo 1D\n",
    "        fc_units = base_filters*32  # 16 * 2**5 = 512 quando base_filters=16\n",
    "        self.fc1 = layers.Dense(fc_units, activation='relu')\n",
    "        self.drop = layers.Dropout(0.4)\n",
    "        self.out = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs, training=training)\n",
    "        x = self.conv2(x, training=training)\n",
    "        x = self.conv3(x, training=training)\n",
    "        x = self.conv_last(x, training=training)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        return self.out(x)\n",
    "\n",
    "# utilitário rápido para comparar contagem de parâmetros\n",
    "def params_count(model, example_input_shape):\n",
    "    m = model\n",
    "    m.build((None,)+example_input_shape)\n",
    "    return m.count_params()\n",
    "\n",
    "# exemplo de uso (instancie e veja parâmetros)\n",
    "cnn1 = CNN1D_fixed(input_shape=(16000,1))\n",
    "cnn2 = CNN2D_matched(input_shape=(64,157,1))  # 64 mel bins × ~157 frames para 1s@16000 com hop≈100\n",
    "print(\"CNN1D params:\", params_count(cnn1, (16000,1)))\n",
    "print(\"CNN2D params:\", params_count(cnn2, (64,157,1)))\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1D_fixed(input_shape=(16000, 1), num_classes=10)\n",
    "\n",
    "# Compilar\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "              tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5)\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Relatório detalhado\n",
    "print(classification_report(y_test, y_pred_classes, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
